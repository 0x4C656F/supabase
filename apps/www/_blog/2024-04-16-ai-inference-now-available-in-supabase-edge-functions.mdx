---
title: 'AI Inference now available in Supabase Edge Functions'
description: 'AI Inference now available in Supabase Edge Functions'
author: laktek
image: ga-week/ai-inference-now-available-in-supabase-edge-functions/og.png
thumb: ga-week/ai-inference-now-available-in-supabase-edge-functions/thumb.png
categories:
  - product
tags:
  - launch-week
  - database
date: '2024-04-16'
toc_depth: 3
launchweek: 11
---

We are making it super easy to run AI models within Supabase Edge Functions. A new built-in API is now available within Edge Runtime, so you can do model inferences in just 2 lines:

```jsx
// Instantiate a new inference session
const session = new Supabase.ai.Session('gte-small')

// then use the session to run inference on a prompt
const output = await session.run('Luke, I am your father')

console.log(output)
// [ -0.047715719789266586, -0.006132732145488262, ...]
```

With this new API you can:

- Generate embeddings using models like `gte-small` to store and retrieve with pgvector. This is available today.
- Inference, using models like `llama2` and `mistral` for GenAI workloads. This will be progressively rolled out as we get our hands on more GPUs.

In our previous Launch Week we announced support for AI inference [via Transformers.js](https://supabase.com/blog/hugging-face-supabase#hugging-face-with-edge-functions). This was a good start but had some shortcoming: it takes time to “boot” the Edge Function because it needs to instantiate a WASM runtime and build the inference pipeline. We increased CPU limits to mitigate these drawbacks, but we knew we wanted a better Developer Experience.

In this post we'll cover some of the improvements to remove cold starts using [Ort](https://github.com/pykeio/ort) and how we're adding inference support using [Ollama](https://ollama.com/).

<div className="video-container">
  <iframe
    className="w-full"
    src="https://www.youtube-nocookie.com/embed/w4Rr_1whU"
    title="Decoupled storage and compute in Postgres"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; fullscreen; gyroscope; picture-in-picture; web-share"
    allowfullscreen
  />
</div>

## Generating Text Embeddings in Edge Functions

Embeddings capture the "relatedness" of text, images, video, or other types of information [[read more](https://www.notion.so/Introducing-AI-inference-in-Edge-Functions-3bd7b1a34422404893b88644b1c89951?pvs=21)]. Embeddings are stored in the database as an array of floating point numbers, known as vectors. Since we [released pgvector](https://supabase.com/blog/openai-embeddings-postgres-vector) on the platform, Postgres has become a popular vector database. You can use pgvector to [store and query embeddings in Postgres](https://supabase.com/docs/guides/ai/vector-columns#querying-a-vector--embedding) and use them for Retrieval Augmented Generation (RAG) and Semantic Search. This release solves a few technical challenges for developers who want to use pgvector.

We've added a new built-in API to Edge Runtime, `Supabase.ai`. This is similar to other runtime APIs like `Deno.serve` or `Deno.env` except only available within Supabase's Edge Runtime:

```jsx
// Instantiate a new inference session
// This can be done once in function
const session = new Supabase.ai.Session('gte-small')

// then use the session to run inference on a prompt
const prompt = 'The sky is blue'
const output = await session.run(prompt, { mean_pool: true, normalize: true })
```

### Integrated pgvector experience

Edge Functions are tightly integrated with the rest of the [Supabase Stack](https://supabase.com/docs/guides/getting-started/architecture), making it a breeze to work with [pgvector](https://supabase.com/docs/guides/database/extensions/pgvector) for storing and querying embeddings. You can see this in action in our [semantic search demo on GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions).

You can now utilize [database webhooks](https://supabase.com/docs/guides/database/webhooks) to automatically generate embeddings whenever a new row is inserted into a database table.

{/* ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/165a3c21-8e38-4d4b-831a-f9bb88a3262b/8b9f49e5-e12a-4c31-a008-9a3c512ee78c/Untitled.png) */}

![Bootstrap getting started](/images/blog/ga-week/ai-inference-now-available-in-supabase-edge-functions/text-insert-example.svg)

Because embedding creation is a compute-intensive task, it makes sense to offload the work from your database. Edge Functions are the perfect “background worker”.

### Technical architecture

{/* ![Screenshot 2024-04-16 at 7.01.44 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/165a3c21-8e38-4d4b-831a-f9bb88a3262b/fe1f2676-b065-4db3-a436-c4d52809d17d/Screenshot_2024-04-16_at_7.01.44_PM.png) */}

Embedding generation uses the [ONNX runtime](https://onnxruntime.ai/) under the hood. This is a cross-platform inferencing library that supports multiple execution providers from CPU to specialized GPUs.

Libraries like `transformers.js` also use ONNX runtime which, in the context of Edge Functions, runs as a WASM module, which can be slow during the instantiation process.

To solve this, we built a native extension in Edge Runtime that enables using ONNX runtime via the Rust interface. This was made possible thanks to an excellent Rust wrapper called [Ort](https://github.com/pykeio/ort):

![Bootstrap getting started](/images/blog/ga-week/ai-inference-now-available-in-supabase-edge-functions/infra-01.svg)

{/* ![Screenshot 2024-04-16 at 6.57.03 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/165a3c21-8e38-4d4b-831a-f9bb88a3262b/9015b93b-cb03-44e8-874c-7cb2356a47c0/Screenshot_2024-04-16_at_6.57.03_PM.png) */}

Embedding generation is fairly lightweight compared to LLM workloads, so it can run on a CPU without hardware acceleration.

### Availability: open source embeddings

Embeddings models are available on Edge Functions today. We currently support `[gte-small](https://huggingface.co/Supabase/gte-small)` and we'll add more embeddings models based on user feedback.

Embedding generation via `Supabase.ai` API is available today for all Edge Functions users in both local, hosted, and self-hosted platforms.

### Lower costs

Generating embeddings in an Edge Function doesn't cost anything extra: we still charge on CPU usage. A typical embedding generation request should run in less than a 1s, even from a cold start. Typically it won't use more than 100-200ms of CPU time.

Proprietary LLMs like OpenAI and Claude provide [APIs](https://platform.openai.com/docs/guides/embeddings) to generate text embeddings, charging per token. For example, OpenAI's `text-embedding-3-small` cost $0.02/1M tokens at the time of writing this post.

Open source text embedding models provide similar performance to OpenAI's paid models. For example, the `gte-small` model, which operates on 384 dimensions, has an average of 61.36 compared to OpenAI's `text-embedding-3-small`, which is at 62.26 on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), and they perform search faster with [fewer dimensions](https://supabase.com/blog/fewer-dimensions-are-better-pgvector).

With Supabase Edge Functions, you can generate text embeddings 10x cheaper than OpenAI embeddings APIs.

(Math)

## Large Language Models in Supabase Edge Functions

Embedding generation only a part of the solution. Typically you need an LLM (like OpenAI's GPT-3.5) to generate human-like interactions. We're working with Ollama to make this possible with Supabase: local development, self-hosted, and on the platform.

### Open source inference models

We are excited to announce experimental support for Llama & Mistral with `Supabase.ai` API.

<video width="99%" autoPlay loop muted playsInline controls={true}>
  <source
    src="https://xguihxuzqibwxjnimxev.supabase.co/storage/v1/object/public/videos/docs/guides/edge-functions-inference.mp4"
    type="video/mp4"
  />
</video>

The API is simple to use, with support for streaming responses:

```jsx
// Mistral with streaming
const session = new Supabase.ai.Session('mistral')

Deno.serve(async (req: Request) => {
  const params = new URL(req.url).searchParams
  const prompt = params.get('prompt') ?? ''
  const output = await session.run(prompt, { stream: true })

  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder()
      for await (const chunk of output) {
        controller.enqueue(encoder.encode(chunk.response ?? ''))
      }
    },
  })

  return new Response(stream, {
    headers: new Headers({
      'Content-Type': 'text/event-stream',
      Connection: 'keep-alive',
    }),
  })
})
```

### Technical architecture

LLM models are challenging to run directly via ONNX runtime on CPU. For these, we are using a GPU-accelerated [Ollama](https://ollama.com/) server under the hood:

![Bootstrap getting started](/images/blog/ga-week/ai-inference-now-available-in-supabase-edge-functions/infra-02.svg)

{/* ![rough rough](https://prod-files-secure.s3.us-west-2.amazonaws.com/165a3c21-8e38-4d4b-831a-f9bb88a3262b/a9ee7f17-3a99-49e1-9331-b101442ceeac/Screenshot_2024-04-16_at_7.04.30_PM.png) */}

We think this is a great match: the Ollama team have worked hard to ensure that the local development experience is great, and we love development environments that can be run without internet access (for those who enjoy programming on planes).

As a Supabase developer, you don't have to worry about deploying models and managing GPU instances - simply use a serverless API to get your job done.

### Availability: open source embeddings

Access to open-source LLMs is currently invite-only while we manage demand for the GPU instances. Please [get in touch](https://forms.supabase.com/supabase.ai-llm-early-access) if you need early access.

## Extending model support

We plan to extend support for more models. [Let us know](https://forms.supabase.com/supabase.ai-llm-early-access) which models you want next. We're looking to support fine-tuned models too!

## Getting started

Check out the Supabase docs today to get started with the AI models:

- Edge Functions: [supabase.com/docs/guides/functions](https://supabase.com/docs/guides/functions)
- Vectors: https://supabase.com/docs/guides/ai
